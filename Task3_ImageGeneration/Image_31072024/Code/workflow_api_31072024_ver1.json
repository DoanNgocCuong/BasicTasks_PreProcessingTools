{
  "1": {
    "inputs": {
      "weight": 0.9,
      "weight_type": "style transfer",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "K+V",
      "model": [
        "5",
        0
      ],
      "ipadapter": [
        "5",
        1
      ],
      "image": [
        "3",
        0
      ],
      "clip_vision": [
        "4",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "2": {
    "inputs": {
      "ckpt_name": "dreamshaperXL_lightningDPMSDE.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "3": {
    "inputs": {
      "image": "InputImag.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "4": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "5": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "2",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "6": {
    "inputs": {
      "seed": 296408152173339,
      "steps": 30,
      "cfg": 6,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "1",
        0
      ],
      "positive": [
        "8",
        0
      ],
      "negative": [
        "9",
        0
      ],
      "latent_image": [
        "7",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "7": {
    "inputs": {
      "width": 1512,
      "height": 1008,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "8": {
    "inputs": {
      "text": "commercial illustration, flat illustration, animation still, corporate animation style, wikihow illustration, quality illustration, 2d animation, professional illustration, cartoon still, digital 2d animation, cg animation, animated still, illustrations, cartoon illustration, high quality, color block, 4k, simple detail, simple background, (flat illustration:1.2), no lines, (vector illustration:1.3), adobe illustration, FOCUS ON 2 people: Sam and Taylor, focus on their friendly interaction, 2-3 other people in the class meeting atmosphere, no laptop in image, (simple color:1.4), (flat style:1.4),(flat illustration style:1.4), <lora:Flat_Corporate_Style:1>, <lora:Fresh Ideas@pixar style_SDXL.safetensors:0.7>, <lora:Flat style:1.4> Flat style",
      "clip": [
        "2",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "9": {
    "inputs": {
      "text": "detailed, deformed, low quality, intricate, realistic, photo, \ndeformed hands, extra fingers, fused fingers, missing fingers, extra limbs, deformed feet, extra toes, missing toes, malformed limbs, disproportionate limbs, distorted anatomy, anatomical errors",
      "clip": [
        "2",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "10": {
    "inputs": {
      "samples": [
        "6",
        0
      ],
      "vae": [
        "2",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "12": {
    "inputs": {
      "anything": [
        "10",
        0
      ]
    },
    "class_type": "easy cleanGpuUsed",
    "_meta": {
      "title": "Clean GPU Used"
    }
  },
  "15": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 26451546633874,
      "steps": 20,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "10",
        0
      ],
      "model": [
        "16",
        0
      ],
      "clip": [
        "16",
        1
      ],
      "vae": [
        "16",
        2
      ],
      "positive": [
        "16",
        3
      ],
      "negative": [
        "16",
        4
      ],
      "bbox_detector": [
        "19",
        0
      ],
      "sam_model_opt": [
        "18",
        0
      ],
      "segm_detector_opt": [
        "17",
        1
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "16": {
    "inputs": {
      "basic_pipe": [
        "20",
        0
      ]
    },
    "class_type": "FromBasicPipe",
    "_meta": {
      "title": "FromBasicPipe"
    }
  },
  "17": {
    "inputs": {
      "model_name": "segm/person_yolov8m-seg.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "18": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "19": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "20": {
    "inputs": {
      "model": [
        "2",
        0
      ],
      "clip": [
        "2",
        1
      ],
      "vae": [
        "2",
        2
      ],
      "positive": [
        "8",
        0
      ],
      "negative": [
        "9",
        0
      ]
    },
    "class_type": "ToBasicPipe",
    "_meta": {
      "title": "ToBasicPipe"
    }
  },
  "21": {
    "inputs": {
      "images": [
        "15",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  }
}